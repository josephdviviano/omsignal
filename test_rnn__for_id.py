# -*- coding: utf-8 -*-
"""test_RNN _for_Id.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vrUHDQ86lr6FKWIKT9vxogV2FqWJYRML
"""

import torch
from torch import nn
from torch.utils import data
import utils
import models
import torchvision.transforms as transforms

# torch.manual_seed(1)    # reproducible

# Hyper Parameters
EPOCH = 10               # train the training data n times, to save time, we just train 10 epoch
BATCH_SIZE = 64
TIME_STEP = 88          # rnn time step /  sequence lengh racine
INPUT_SIZE = 88         # rnn input size / sequence lengh racine
LR = 0.01               # learning rate

CUDA = torch.cuda.is_available()
CONFIG = utils.read_config()

params = {
        'batch_size': CONFIG['dataloader']['batch_size'],
        'shuffle': CONFIG['dataloader']['shuffle'],
        'num_workers': CONFIG['dataloader']['num_workers']
    }

# set up Dataloaders
train_data = utils.Data(augmentation=True)
valid_data = utils.Data(train=False, augmentation=True)
train_load = data.DataLoader(train_data, **params)
valid_load = data.DataLoader(valid_data, **params)
# configure cuda
device = torch.device("cuda:0" if CUDA else "cpu")
torch.backends.cudnn.benchmark = True

# TODO do I need this: Data Loader for easy mini-batch return in training
train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)


class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()

        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns
            input_size=INPUT_SIZE,
            hidden_size=64,         # rnn hidden unit
            num_layers=1,           # number of rnn layer
            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)
        )

        self.out = nn.Linear(64, 10)

    def forward(self, x):
        # x shape (batch, time_step, input_size)
        # r_out shape (batch, time_step, output_size)
        # h_n shape (n_layers, batch, hidden_size)
        # h_c shape (n_layers, batch, hidden_size)
        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state

        # choose r_out at the last time step
        out = self.out(r_out[:, -1, :])
        return out


rnn = RNN()
print(rnn)

optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters
loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted

# training and testing
for epoch in range(EPOCH):
    for step, (b_x, b_y) in enumerate(train_loader):        # gives batch data
        b_x = b_x.view(-1, 88, 88)              # reshape x to (batch, time_step, input_size)

        output = rnn(b_x)                               # rnn output
        loss = loss_func(output, b_y)                   # cross entropy loss
        optimizer.zero_grad()                           # clear gradients for this training step
        loss.backward()                                 # backpropagation, compute gradients
        optimizer.step()                                # apply gradients

        if step % 50 == 0:
            valid_output = rnn(X_valid)                   # (samples, time_step, input_size)
            pred_y = torch.max(valid_output, 1)[1].data.numpy()
            accuracy = float((pred_y == y_valid).astype(int).sum()) / float(y_valid.size)
            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| valid accuracy: %.2f' % accuracy)

# print 32 predictions from valid data
valid_output = rnn(X_valid[:32].view(-1, 88, 88))
pred_y = torch.max(valid_output, 1)[1].data.numpy()
print(pred_y, 'prediction number')
print(y_valid[:32], 'real number')